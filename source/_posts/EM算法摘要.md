---
title: EM算法小结
date: 2018-07-13 10:03:59
tags: [EM]
categories:
- 算法
---

EM算法是机器学习领域常见的一种算法了，由于最近又多次看到跟其相关的东西，所以做一个小结以备忘。

个人认为EM中最终的是理解参数和隐状态这两个点。此处引用李航《统计学习方法》中的例子来做说明。

> 假设有三枚硬币A、B、C，这些硬币正面出现的概率分别是 ππ 、pp 、qq 。进行如下掷硬币试验： 先掷A，如果A是正面则再掷B，如果A是反面则再掷C。对于B或C的结果，如果是正面则记为1，如果是反面则记为0。进行 NN 次独立重复实验，得到结果。现在只能观测到结果，不能观测到掷硬币的过程，估计模型参数 θ=(π,p,q) 。 

在这个问题中，实验结果是可观测数据$ Y=(y_1,...,y_N)$，硬币A的结果是不可观测数据 $Z=(z_1,...,z_N)$且 z 只有两种可能取值1和0。

我们需要求得$\hat{\theta}=arg max_{\theta}{logP(Y|\theta)}$这个问题没有解析解，只能迭代求解，因为$p(y|\theta)=\sum_{Z}P(Z|\theta)P(Y|Z,\theta)$需要依赖于隐状态。所以就想出了一种迭代求值的方式

先假设一个初始参数$\theta^0$

E步被称为期望步，通俗点说就是利用上一步的参数$\theta^{i-1}$，以此参数求出隐状态的条件概率概率，严谨一点定义的话我们需要传说中的Q函数：完全数据的对数似然函数（观测数据和隐变量数据构成完全数据）关于在给定观测数据Y和当前参数$\theta^{i-1}$下对未观测数据Z的条件概率分布的期望。公式如下：
$$
\begin{aligned}
Q(\theta,\theta^{(i)})&=E_Z[logP(Y,Z|\theta)]\\
&=\sum_{Z}logP(Y,Z|\theta)P(Z|Y,\theta^{(i-1)})
\end{aligned}
\tag{1}\label{1}
$$
这里，$P(Z|Y,\theta^{(i-1)})$是在给定观测数据Y和上一轮参数估计$\theta^{(i-1)}$下隐变量数据Z的条件概率分布，也就是我们E步求得最重要的一个值，三硬币模型中就是求得$y_j$来自硬币B的概率（已知y和$\theta$求得硬币A证明朝上的概率）

M步：求使$Q(\theta,\theta^{(i)})$极大化的$\theta$，更新$\theta^{(i)}=arg \max_{\theta}Q(\theta,\theta^{(i-1)})$，在三硬币模型中也就是知道每次硬币来自硬币B的概率$\mu_i$的情况下更新参数$\theta$，二项分布最优值为$\frac{k}{n}$时，$\pi^{(i)}=\frac{1}{n}\sum_{j=1}^{n}\mu_j^{i}$，$p^{(i)}=\frac{\sum_{j=1}^{n}\mu_j^{i}y_i}{\sum_{j=1}^{n}\mu_j^{i}}$

至于为什么不断迭代Q函数就可以获取有隐状态下的最大似然，用Jensen不等式证明，这里不展开。

再通过几个例子加强下

**例1:**现在一个班里有100个男生，100个女生。我们假定男生的身高服从正态分布 ，女生的身高则服从另一个正态分布：  。假如男女是分开的，这时候我们可以用极大似然法（MLE），分别通过这100个男生和100个女生的样本来估计这两个正态分布的参数。但现在情况要更复杂一点，就是这100个男生和100个女生混在一起了。我们拥有100个人的身高数据，却不知道这100个人每一个是男生还是女生。

那么我们一个样本是男生还是女生就是一个隐状态。

E步，我们利用先验知识之类的先假设出男生和女生的正态分布参数（或者来自于迭代的上一步），这样我们就可以估算出一个样本是男生还是女生

M步，我们区分出了每个样本是男生还是女生，就可以更新正态分布参数

**例2：**对于常见的k-means算法，我们认为每个点属于哪个模型是隐状态，聚类中心点是参数

E步，假设一些聚类中心点，求取隐状态每个点的类别

M步，根据隐状态更新中心点



------

### 总结

理解EM算法最重要的个人认为是找到隐状态和参数，E步根据参数求取隐状态，M步根据隐状态求取是Q函数最大的参数。